{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import jsonlines\n",
    "import csv\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\" # the device to load the model onto\n",
    "\n",
    "model_id = \"/users/wangdongnian/outputs/ckpt/qwen2_7b_xf_v3-20240705-195808/iter_0000429/huggingface_format\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=device,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_path = \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtai-llms/users/wangdongnian/outputs/ckpt/qwen2_7b_xf_lora_r64_a16_epoch5/last_ft_model\"\n",
    "# model = PeftModel.from_pretrained(model, model_id=lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwen_infer(dialogue, model, tokenizer, prompt_text=prompt_exect):\n",
    "    content = prompt_text.replace(\"##Content##\", dialogue)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # try:\n",
    "    #     if '```json' in response:\n",
    "    #         response = response.replace('```json', '').replace('```', '')\n",
    "    #     # response = json.loads(response)\n",
    "    # except json.JSONDecodeError as e:\n",
    "    #     print(\"JSONDecodeError : \" + response)\n",
    "    #     raise e\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all_json_in_text_to_dict(text):\n",
    "    \"\"\"Extracts JSON objects from the text.\"\"\"\n",
    "    dicts, stack = [], []\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == '{':\n",
    "            stack.append(i)\n",
    "        elif text[i] == '}':\n",
    "            begin = stack.pop()\n",
    "            if not stack:\n",
    "                dicts.append(json.loads(text[begin:i+1]))\n",
    "    return dicts\n",
    "\n",
    "answer = qwen_infer(test_data.iloc[0][\"chat_text\"], model, tokenizer, prompt_exect)\n",
    "print(answer)\n",
    "infos = convert_all_json_in_text_to_dict(answer.replace(\"\\'\", \"\\\"\"))\n",
    "# infos = json.load(answer)\n",
    "print(infos)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
